{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have put in lots of work to make this notebook\n",
    "-  refering to the important concepts mentioned in the **michael nielsen deep learning book** (chapter 1, chapter 2, chapter 3)\n",
    "- I have tried my best to make a neural network class from the beginning my own without copying the code in the **michael nielsen** book.\n",
    "- i have wanted to make a class that is similar in structure as the tensorflow library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At the beginning i used a generated data using the **make_classification** library to test and debug my framework before trying it on the minist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X, Y = make_classification(n_samples = n, n_features=7,\n",
    "                           n_redundant=0, n_informative=7, flip_y=0.0, n_classes=3)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.int32)\n",
    "\n",
    "train_x, test_x = np.split(X, [n*8//10])\n",
    "train_labels, test_labels = np.split(Y, [n*8//10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.9715028  -2.111734    2.6699836  -0.89083135 -4.324263    0.09723637\n",
      "  -0.68138725]\n",
      " [-2.4533546  -1.1386522   0.14909688  0.9929887  -1.5600386  -1.7178034\n",
      "   0.59493536]\n",
      " [-3.0627162  -0.7382625   1.085122   -0.42586046  2.318875   -0.84472156\n",
      "   0.43813038]\n",
      " [ 1.3751217   1.7328203   1.7510338  -1.2073286   0.01872577  0.15517184\n",
      "  -0.5734819 ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0:4])\n",
    "# print(train_labels[10:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a layer class\n",
    "\n",
    "- representing the linear layers of the neural network\n",
    "\n",
    "- Its functions:\n",
    "    1. forward(x): Computes the weighted input (z) and applies the sigmoid activation if enabled.\n",
    "    2. backward(dz): Computes gradients for weights (dW), biases (db), and the input (dx) using the chain rule.\n",
    "    3. update(lr): Updates the weights and biases using the computed gradients and the learning rate.\n",
    "\n",
    "- have made a built in sigmoid boolean in each layer that can activate the sigmoid functions for the outputs of the neurons in the layer if set to TRUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50962547  0.16957474  0.08069371 -0.27969603  0.3778904  -0.07800394\n",
      "   0.2465736 ]\n",
      " [-0.54821106 -0.10888825  0.44066916  0.10480896 -0.55485027 -0.22940648\n",
      "   0.04730613]\n",
      " [-0.8473482  -0.09613679  0.02987103 -0.33097631  0.23691839  0.21238879\n",
      "   0.30154372]]\n"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout, sigmoid = False):\n",
    "        self.sigmoid = sigmoid\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))\n",
    "        self.b = np.zeros((1,nout))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = np.dot(x, self.W.T) + self.b\n",
    "\n",
    "        if self.sigmoid: return 1.0/(1.0+np.exp(-z))\n",
    "        return z\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dx = np.dot(dz, self.W)\n",
    "        dW = np.dot(dz.T, self.x)\n",
    "        db = dz.sum(axis=0)\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        return dx\n",
    "    \n",
    "    def update(self,lr):\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db\n",
    "    \n",
    "    \n",
    "mod = Layer(7,3, sigmoid = True)\n",
    "# net.forward(train_x[0])\n",
    "print(mod.W)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "as described in the book(chapter 3)\n",
    "\n",
    "- The idea of softmax is to define a new type of output layer for our neural networks. It begins in the same way as with a sigmoid layer. However, we don't apply the sigmoid function to get the output. Instead, in a softmax layer we apply the so-called softmax function which changes the output of the neuron as follows:  \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- So basically it changes the output of the neuron into a probability\n",
    "\n",
    "> Note  I have used Softmax instead of Sigmoid in my training as it was working better for me "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def forward(self,z):\n",
    "        self.z = z\n",
    "        zmax = z.max(axis=1,keepdims=True)\n",
    "        expz = np.exp(z-zmax)\n",
    "        Z = expz.sum(axis=1,keepdims=True)\n",
    "        return expz / Z\n",
    "    \n",
    "    def backward(self,dp):\n",
    "        p = self.forward(self.z)\n",
    "        pdp = p * dp\n",
    "        return pdp - p * pdp.sum(axis=1, keepdims=True)\n",
    "\n",
    "softmax = Softmax()\n",
    "z2 = softmax.forward(mod.forward(train_x[0:10]))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy cost function\n",
    "\n",
    "- As mentioned in the book (chapter 3) Cross Entropy is one of the best loss functions we can use in a multi layer perceptron\n",
    "\n",
    "- The cross-entropy cost function has the benefit that, unlike the quadratic cost, it avoids the problem of learning slowing down.\n",
    "\n",
    "- The cross-entropy cost function is defined by\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.318873"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self,p,y):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        p_of_y = p[np.arange(len(y)), y]\n",
    "        log_prob = np.log(p_of_y)\n",
    "        return -log_prob.mean()\n",
    "    \n",
    "    def backward(self,loss):\n",
    "        dlog_softmax = np.zeros_like(self.p)\n",
    "        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)\n",
    "        return dlog_softmax / self.p\n",
    "    \n",
    "cross_ent_loss = CrossEntropyLoss()\n",
    "p = softmax.forward(train_x[0:10])\n",
    "\n",
    "print(train_labels[0:3])\n",
    "# p = np.array([[0, 0, 1],\n",
    "#               [1, 0, 0],\n",
    "#               [0,0,1]])\n",
    "\n",
    "cross_ent_loss.forward(p,train_labels[0:3])\n",
    "# cross_ent(z2, train_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlinearity functions\n",
    "\n",
    "- In multilayer nertworks we have to put functions between layers that introduce unlinearity to the layer instead of all the layers acting as just one layer.  \n",
    "> I still don't really understand how these functions do it\n",
    "\n",
    "- Two of the most used activation functions are **Tanh**, **Relu**\n",
    "#### Tanh\n",
    "- it calculate the tanh for the output   \n",
    "#### Relu\n",
    "- it return the maximum between the uotput and some number as zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self,x):\n",
    "        y = np.tanh(x)\n",
    "        self.y = y\n",
    "        return y\n",
    "    def backward(self,dy):\n",
    "        return (1.0-self.y**2)*dy\n",
    "    \n",
    "class Relu:\n",
    "    def forward(self, x):\n",
    "        self.y = np.maximum(0, x) \n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return (self.y > 0) * dy \n",
    "    \n",
    "# T = Tanh()\n",
    "# T.forward()\n",
    "# r = Relu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2960994958853034\n"
     ]
    }
   ],
   "source": [
    "z = mod.forward(train_x[0:10])\n",
    "p = softmax.forward(z)\n",
    "loss = cross_ent_loss.forward(p,train_labels[0:10])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin1 = Layer(7,7)\n",
    "# lin2 = Layer(7,2)\n",
    "# softmax = Softmax()\n",
    "# cross_ent_loss = CrossEntropyLoss()\n",
    "\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# pred = np.argmax(lin2.forward(lin1.forward(train_x)),axis=1)\n",
    "# acc = (pred==train_labels).mean()\n",
    "# print(acc)\n",
    "\n",
    "\n",
    "# batch_size=4\n",
    "# epochs = 4\n",
    "# for j in range(epochs):\n",
    "#     for i in range(0,len(train_x),batch_size):\n",
    "#         xb = train_x[i:i+batch_size]\n",
    "#         yb = train_labels[i:i+batch_size]\n",
    "        \n",
    "#         # forward pass\n",
    "#         z1 = lin1.forward(xb)\n",
    "#         z = lin2.forward(z1)\n",
    "\n",
    "#         p = softmax.forward(z)\n",
    "#         loss = cross_ent_loss.forward(p,yb)\n",
    "        \n",
    "#         # backward pass\n",
    "#         dp = cross_ent_loss.backward(loss)\n",
    "#         dz2 = softmax.backward(dp)\n",
    "#         dz = lin2.backward(dz2)\n",
    "#         lin2.update(learning_rate)\n",
    "#         dx = lin1.backward(dz)\n",
    "#         lin1.update(learning_rate)\n",
    "        \n",
    "#     pred = np.argmax(lin1.forward(train_x),axis=1)\n",
    "#     acc = (pred==train_labels).mean()\n",
    "#     print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class  \n",
    "\n",
    "Now i will make a class that will represent our NN model by adding the layer we want together\n",
    "\n",
    "- It will have the following functions \n",
    "\n",
    "1. forward: makes predictions for our model \n",
    "2. backward: Makes the back propagation for our model \n",
    "**Back propagation** is the process of knowing how to adjust the wights and biases in the network to reduce the loss function output. It was explained in **Chapter Two**. it works by making backward derivatvies using the chain rule through out the whole model layers.\n",
    "\n",
    "> The backward functions in the model was understood and took from the concepts of the functions in [this REPO](https://github.com/microsoft/etrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self,l):\n",
    "        self.layers.append(l)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self,z):\n",
    "        for l in self.layers[::-1]:\n",
    "            z = l.backward(z)\n",
    "        return z\n",
    "    \n",
    "    def update(self,lr):\n",
    "        for l in self.layers:\n",
    "            if 'update' in l.__dir__():\n",
    "                l.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "- Defining a function that takes data with its corresponding labels and strt training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_acc(mod, x,y,loss=CrossEntropyLoss()): ## Return the cost, accuaracy\n",
    "    p = mod.forward(x)\n",
    "    l = loss.forward(p,y)\n",
    "    pred = np.argmax(p,axis=1)\n",
    "    acc = (pred==y).mean()\n",
    "    return l,acc\n",
    "\n",
    "def train_model(model, train_data, valid_data, loss=CrossEntropyLoss(),epochs = 5, batch_size=16, lr=0.01):\n",
    "    for x in range (epochs):\n",
    "        for j in range(0,len(train_x),batch_size):\n",
    "            i = np.random.choice(range(0,len(train_x)-batch_size-1) )\n",
    "            xb = train_x[i:i+batch_size]\n",
    "            yb = train_labels[i:i+batch_size]\n",
    "\n",
    "            p = model.forward(xb)\n",
    "            l = loss.forward(p,yb)\n",
    "            dp = loss.backward(l)\n",
    "            dx = model.backward(dp)\n",
    "            model.update(lr)\n",
    "\n",
    "        losss, acc = get_loss_acc(model, train_data[0],train_data[1])\n",
    "        print(f\"\\nepochs: {x+1}, accuracy: {acc}, loss:{losss} \",end =\" \")\n",
    "        losss, acc = get_loss_acc(model, valid_data[0],valid_data[1])\n",
    "        print(f\"Validation accuracy: {acc}, Validation loss:{losss} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=0.375, accuracy=2.3371403184710347: \n",
      "\n",
      "epochs: 1, accuracy: 0.375, loss:1.9159618118865758  Validation accuracy: 0.5, Validation loss:1.3940783827631038 \n",
      "\n",
      "\n",
      "epochs: 2, accuracy: 0.425, loss:1.587068386321603  Validation accuracy: 0.5, Validation loss:1.1629232281768278 \n",
      "\n",
      "\n",
      "epochs: 3, accuracy: 0.4375, loss:1.3391755131500758  Validation accuracy: 0.5, Validation loss:1.0176446305777618 \n",
      "\n",
      "\n",
      "epochs: 4, accuracy: 0.4875, loss:1.162060125015866  Validation accuracy: 0.5, Validation loss:0.9331793373751648 \n",
      "\n",
      "\n",
      "epochs: 5, accuracy: 0.55, loss:1.0110706761760344  Validation accuracy: 0.55, Validation loss:0.8233795881331195 \n",
      "\n",
      "\n",
      "epochs: 6, accuracy: 0.5875, loss:0.899450580213014  Validation accuracy: 0.6, Validation loss:0.77780405210772 \n",
      "\n",
      "\n",
      "epochs: 7, accuracy: 0.625, loss:0.8573997806397495  Validation accuracy: 0.6, Validation loss:0.766422415227524 \n",
      "\n",
      "\n",
      "epochs: 8, accuracy: 0.6375, loss:0.822388947894359  Validation accuracy: 0.6, Validation loss:0.753445161204735 \n",
      "\n",
      "\n",
      "epochs: 9, accuracy: 0.675, loss:0.8056522479153859  Validation accuracy: 0.55, Validation loss:0.7562613837330833 \n",
      "\n",
      "\n",
      "epochs: 10, accuracy: 0.6375, loss:0.7992346178587095  Validation accuracy: 0.55, Validation loss:0.7585190038975635 \n",
      "\n",
      "\n",
      "epochs: 11, accuracy: 0.6875, loss:0.7867411465908185  Validation accuracy: 0.55, Validation loss:0.7559632290313651 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = Model()\n",
    "model1.add(Layer(7,3))\n",
    "model1.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "losss, acc = get_loss_acc(model1, train_x,train_labels)\n",
    "print(f\"Initial loss={acc}, accuracy={losss}: \")\n",
    "\n",
    " \n",
    "train_model(model1,(train_x, train_labels),(test_x, test_labels), epochs=11, lr = 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works good enough\n",
    "\n",
    "- we can see the accuracy increasing and the loss decreasing\n",
    "\n",
    "### Now i will try to make a larger model using my framework and train it on the minist data sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(train_x, train_labels), (test_x, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_x = train_x.reshape(60000, 28 * 28)\n",
    "test_x = test_x.reshape(10000, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i reshaped the data set to convert it into a 1D vector which my model can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.499090246396412, accuracy=0.10285: \n",
      "\n",
      "epochs: 1, accuracy: 0.66715, loss:1.4319905056684998  Validation accuracy: 0.6791, Validation loss:1.4182191129234991 \n",
      "\n",
      "\n",
      "epochs: 2, accuracy: 0.7178, loss:1.332650129624932  Validation accuracy: 0.7262, Validation loss:1.326334270707869 \n",
      "\n",
      "\n",
      "epochs: 3, accuracy: 0.7322833333333333, loss:1.296286742999343  Validation accuracy: 0.7377, Validation loss:1.2827135682284705 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = Model()\n",
    "model2.add(Layer(28*28,36))\n",
    "model2.add(Tanh())\n",
    "# model2.add(Relu)\n",
    "model2.add(Layer(36,10))\n",
    "model2.add(Tanh())\n",
    "\n",
    "model2.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "losss, acc = get_loss_acc(model2, train_x,train_labels)\n",
    "print(f\"Initial loss={losss}, accuracy={acc}: \")\n",
    "\n",
    " \n",
    "train_model(model2,(train_x, train_labels),(test_x, test_labels), epochs=5, lr = 0.02, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrameWork works Good "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
